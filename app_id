#!/usr/bin/env python3
# -*- encoding: utf-8 -*-
'''
@File    :   ttt.py
@Contact :   258770530@qq.com.com
@Modify Time      @Author        @Version    @Desciption
------------      -------        --------    -----------
2019/4/25 16:39   Jacques Lim    1.0         None
'''
from bs4 import BeautifulSoup
import requests
from fake_useragent import UserAgent
import random

ips = [
    "196.13.208.23:8080",
    "47.99.113.175:8118",
    "185.82.212.95:8080",
    "219.223.222.6:8123",
    "221.210.120.153:54402",
    "60.205.202.3:3128",
    "87.139.168.220:8080",
    "95.88.12.230:3128",
    "111.198.154.116:8888",
    "94.130.20.85:31288"
    ]

def get_random_header():
    ua = UserAgent()
    header = {
        "User-Agent": ua.random

    }
    print(header)
    return header

def getHtml(url):
    # ....
    retry_count = 5
    while retry_count > 0:
        # headers = get_random_header()
        # rand_ip = get_proxy_ip()
        proxies = {
            # 'https': 'https://' + rand_ip
        }

        try:
            # html = requests.get(url, headers=headers, proxies=proxies, timeout=10)
            html = requests.get(url, timeout=10)
            if len(html.text) > 0:
                return html
            else:
                getHtml(url)

        except Exception:
            retry_count -= 1
    # 出错5次, 删除代理池中代理
    # ips.remove(rand_ip)
    print('剩余可用IP数量：'+str(len(ips)))
    getHtml(url)

def get_proxy_ip():
    rand_ip = random.choice(ips)
    print(rand_ip)
    return rand_ip

def get_links(url,classname):
    '''
    获取一定格式的所有链接
    :param url: 页面url
    :param classname: 类名为classname
    :return: 所有链接列表
    '''
    res = getHtml(url)
    links = []
    try:
        # print("获取的内容是：" + res.text)
        soup = BeautifulSoup(res.text, 'html.parser')
        all_a = soup.find(class_=classname).find_all('a')
        for one_a in all_a:
            link = one_a.get('href')
            links.append(link)
    except:
        print("获取的HTML内容为空")
        get_links(url, classname)
    return links

alphabet = ['#', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',
            'U', 'V', 'W', 'X', 'Y', 'Z']
# genre_links = get_links('https://itunes.apple.com/cn/genre/ios/id36?mt=8','grid3-column')
# genre_letter_links = []
# for genre_link in genre_links:
#     for letter in alphabet:
#         link = genre_link + '&letter=' + letter
#         genre_letter_links.append(link)
# print(len(genre_letter_links))

def get_pav_last_default_links(link):
    url = link + '&page=300'
    return get_links(url, 'grid3-column')
def get_pagecount(start, end):
    if (end - start) > 1:
        p = start + int((end - start) / 2)
        links = get_links('https://itunes.apple.com/cn/genre/ios-%E5%95%86%E5%8A%A1/id6000?mt=8&letter=B&page=' + str(p), 'grid3-column')
        pav_last_default_links = get_pav_last_default_links('https://itunes.apple.com/cn/genre/ios-%E5%95%86%E5%8A%A1/id6000?mt=8&letter=B')
        print('{}~{}'.format(start, end))
        if links==pav_last_default_links:
            get_pagecount(start, p)
        else:
            get_pagecount(p, end)
    else:
        return end

print(get_pagecount(0, 300))