#!/usr/bin/env python3
# -*- encoding: utf-8 -*-
'''
@File    :   ttt.py
@Contact :   258770530@qq.com.com
@Modify Time      @Author        @Version    @Desciption
------------      -------        --------    -----------
2019/4/25 16:39   Jacques Lim    1.0         None
'''
from bs4 import BeautifulSoup
import requests
from fake_useragent import UserAgent
import random

'''
配置文件路径
genre_links_path : 记录各个分类热门APP链接
genre_letters_links_path: 记录各个分类字母分类链接
genre_letters_pavs_links_path: 记录各个分类字母分类下各个分页的链接
all_pages_links_path: 所有需要爬取APPID的有效页面链接
all_app_links_path: 存储所有APP的链接url
'''
genre_links_path = 'C:/Users/mayn/Desktop/genre_links.txt'
genre_letters_links_path = 'C:/Users/mayn/Desktop/genre_letter_links.txt'
genre_letters_pavs_links_path = 'C:/Users/mayn/Desktop/genre_letter_pav_links.txt'
all_pages_links_path = 'C:/Users/mayn/Desktop/all_pages_links.txt'
all_app_links_path =  'C:/Users/mayn/Desktop/all_app_links.txt'

ips = [
  "119.180.161.22:8060",
  "27.203.244.62:8060",
  "27.208.140.165:8060",
  "47.75.223.130:8080",
  "27.208.70.35:8060",
  "196.13.208.23:8080",
  "47.52.231.140:8080",
  "47.94.230.42:9999",
  "94.130.20.85:31288",
  "221.4.150.7:8181"
]

def get_random_header():
    ua = UserAgent()
    header = {
        "User-Agent": ua.random

    }
    print(header)
    return header

def getHtml(url):
    # ....
    retry_count = 5
    while retry_count > 0:
        headers = get_random_header() #爬取有效链接时不用
        rand_ip = get_proxy_ip() #爬取有效链接时不用
        proxies = {
            'https': 'https://' + rand_ip
        }

        try:
            html = requests.get(url, headers=headers, proxies=proxies, timeout=10)
            # html = requests.get(url, timeout=10)
            if len(html.text) > 0:
                return html
            else:
                getHtml(url)

        except Exception:
            retry_count -= 1
    # 出错5次, 删除代理池中代理
    ips.remove(rand_ip)
    print('剩余可用IP数量：'+str(len(ips)))
    getHtml(url)

def get_proxy_ip():
    rand_ip = random.choice(ips)
    print(rand_ip)
    return rand_ip

def get_links(url,classname):
    '''
    获取一定格式的所有链接
    :param url: 页面url
    :param classname: 类名为classname
    :return: 所有链接列表
    '''
    res = getHtml(url)
    links = []
    try:
        # print("获取的内容是：" + res.text)
        soup = BeautifulSoup(res.text, 'html.parser')
        all_a = soup.find(class_=classname).find_all('a')
        for one_a in all_a:
            link = one_a.get('href')
            links.append(link)
    except:
        print("获取的HTML内容为空")
        get_links(url, classname)
    return links

alphabet = ['#', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',
            'U', 'V', 'W', 'X', 'Y', 'Z']


def get_pav_last_default_links(link):
    '''
    用于处理每个分类下最后一页的数据，记录初始值
    :param link: 每个分类字母的链接
    :return: 返回最后一页的链接数据，最后一页和最后300页的数据是一样的
    '''
    url = link + '&page=300'
    return get_links(url, 'grid3-column')
def get_pagecount(url, start, end):
    '''
    找到每个字母分页下共有多少数字分页
    :param url: 需要分析的链接
    :param start: 开始页数
    :param end: 结束页数
    :return: 最终页数
    '''
    p = start + int((end - start) / 2)
    links = get_links('{}&page={}'.format(url, p), 'grid3-column')
    pav_last_default_links = get_pav_last_default_links(url)
    # print('{}~{}'.format(start, end))
    if links == pav_last_default_links and (end - start) > 1:
        return get_pagecount(url, start, p)

    elif links != pav_last_default_links and (end - start) > 1:
        return get_pagecount(url, p, end)
    else:
        return p

'''
爬取所有有效链接

genre_links = get_links('https://itunes.apple.com/cn/genre/ios/id36?mt=8','grid3-column')
genre_letter_links = []
for genre_link in genre_links:
    with open(genre_links_path, 'a+', encoding='utf8')as f:  #记录各个分类热门APP链接
        f.write(genre_link + '\n')
    for letter in alphabet:
        link = genre_link + '&letter=' + letter
        with open(genre_letters_links_path, 'a+', encoding='utf8')as f: #记录各个分类字母分类链接
            f.write(link + '\n')
        genre_letter_links.append(link)
print(len(genre_letter_links))
for genre_letter_link in genre_letter_links:
    start = 0
    end = get_pagecount(genre_letter_link, 0, 200)
    print(genre_letter_link)
    print('总页数：{}'.format(end))
    for i in range(1,end+1):
        genre_letter_pav_link = genre_letter_link + '&page={}'.format(i)
        with open(genre_letters_pavs_links_path, 'a+', encoding='utf8')as f: #记录各个分类字母分类下各个分页的链接
            f.write(genre_letter_pav_link + '\n')

'''
all_pages_links = []
with open(all_pages_links_path, 'r', encoding='utf8')as f:  # 记录各个分类热门APP链接
    all_pages_links = f.read().split('\n')
for page_link in all_pages_links:
    page_app_links = get_links(page_link, 'grid3-column')
    with open(genre_letters_pavs_links_path, 'a+', encoding='utf8')as f:  # 记录各个分类字母分类下各个分页的链接
        f.write(all_app_links_path + '\n')